{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 图像预处理，采用Ben的算法，剪取中央图像去黑边，为克服光照影响，高斯滤波的sigmaX参数取10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "from keras.utils import Sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "WORKERS = 2\n",
    "CHANNEL = 3\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "IMG_SIZE = 512\n",
    "NUM_CLASSES = 5\n",
    "SEED = 77\n",
    "TRAIN_NUM = 1000 # use 1000 when you just want to explore new idea, use -1 for full train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction. Explore first, train later.\n",
    "\n",
    "Hi everyone! As Aravind Eye Hospital is one of my favorite organization in the world; they take care of poor people's eyes for free with an impressive sustainable business model. I will try my best to contribute something to our community. One intuitive way to improve the performance of our model is to simply improve the quality of input images. In this kernel, I will share two ideas which I hope may be useful to some of you :\n",
    "\n",
    "- **Reducing lighting-condition effects** : as we will see, images come with many different lighting conditions, some images are very dark and difficult to visualize. We can try to convert the image to gray scale, and visualize better. Alternatively, there is a better approach. We can try the method of Ben Graham (last competition's winner)\n",
    "- **Cropping uninformative area**: everyone know this :) Here, I just find the codes from internet and choose the best one for you :)\n",
    "\n",
    "We are going to apply both techniques to both the official data, and the past competition data (shout out @tanlikesmath for creating this dataset! https://www.kaggle.com/tanlikesmath/diabetic-retinopathy-resized . In the updated version, I also try @donkeys' dataset https://www.kaggle.com/donkeys/retinopathy-train-2015 , which is .png which may be have higer image quality than .jpeg format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\n",
    "df_test = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\n",
    "\n",
    "x = df_train['id_code']\n",
    "y = df_train['diagnosis']\n",
    "\n",
    "x, y = shuffle(x, y, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3112,) (3112,) (550,) (550,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f20119d79e8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.15,\n",
    "                                                      stratify=y, random_state=SEED)\n",
    "print(train_x.shape, train_y.shape, valid_x.shape, valid_y.shape)\n",
    "train_y.hist()  # 蓝色 训练集数量\n",
    "valid_y.hist()  # 橙色 验证集数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple picture to explain Diabetic Retinopathy\n",
    "How do we know that a patient have diabetic retinopahy? There are at least 5 things to spot on. Image credit https://www.eyeops.com/\n",
    "![image.png](https://sa1s3optim.patientpop.com/assets/images/provider/photos/1947516.jpeg)\n",
    "\n",
    "\n",
    "From quick investigations of the data (see various pictures below), I found that Hemorrphages, Hard Exudates and Cotton Wool spots are quite easily observed. However, I still could not find examples of Aneurysm or Abnormal Growth of Blood Vessels from our data yet. Perhaps the latter two cases are important if we want to catch up human benchmnark using our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Further improve by auto-cropping\n",
    "\n",
    "To crop out the uninformative black areas which are evident on pic(0,1), pic(0,3) and pic(4,1), we can try auto cropping. I found 4 alternative codes from https://stackoverflow.com/questions/13538748/crop-black-edges-with-opencv and https://codereview.stackexchange.com/questions/132914/crop-black-border-of-image-using-numpy/132934 ... Fortunately one method works perfectly for a gray scale image, but none works on a color image. In this kernel, I modify the method working on gray-scale a bit to make it suitable for a color image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Try Cropping the images\n",
    "\n",
    "I have tested on around 200 images, and the method works great. However, if anybody find the outlier cases which cause the auto crop to fail, please let me know. I think now the eye pictures are very like the moon by the way :)\n",
    "\n",
    "**IMPORTANT UPDATE** on Kernel V.9 I found that there is indeed a case in private test set making the old version of crop function fail. (I spent my 13 submissions until I found this bug) E.g. if there is an adversarial image (super dark) in the private test set, the crop function will crop everything and result in 0 dimension image. I have fixed this bug in this kernel version, but I still could not guarantee whether there are other cases in a private test that will make the crop function fail or not. Update on V11 Now I was able to have a valid LB score with the new crop function, so if anybody still have some submission errors, that is the reason of other bugs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# A Important Update on Color Version of Cropping & Ben's Preprocessing\n",
    "\n",
    "At first, when I wrote this kernel, I could not make a color crop nicely, so I thought that gray scale is a better representation. Now I believe that color version is better, so from this point on I will use color cropping\n",
    "\n",
    "Below is the cropped of the color version. For color version, note that I use argument sigmaX = 30 of cv2.GaussianBlur, where Ben actually used sigmaX = 10 which may have better performance. I just feel that this sigmaX = 30 or sigmaX = 50 make beautiful [sometimes bloody] yellow moon pictures. Just for the purpose of illustration.\n",
    "\n",
    "Please refer to https://www.tutorialkart.com/opencv/python/opencv-python-gaussian-image-smoothing/ .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ben_color(path, sigmaX=10):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = crop_image_from_gray(image)\n",
    "    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX) ,-4 ,128)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old = pd.read_csv('../input/diabetic-retinopathy-resized/trainLabels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25ed6d468814dd68f9af36fdeabb564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"../input/diabetic-retinopathy-resized/ben_preprocessing_sigmaX10\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "for idx, row in tqdm_notebook(df_old.iterrows()):\n",
    "    path=f\"../input/diabetic-retinopathy-resized/resized_train/{row['image']}.jpeg\"\n",
    "    image = load_ben_color(path,sigmaX=10)\n",
    "    Image.fromarray(image).save(os.path.join(save_dir, \"{}.png\".format(row['image'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6922c45947e4a0aa580a71c6028d3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"../input/aptos2019-blindness-detection/train_images_ben_preprocessing_sigmaX10\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "for idx, row in tqdm_notebook(df_train.iterrows()):\n",
    "    path=f\"../input/aptos2019-blindness-detection/train_images/{row['id_code']}.png\"\n",
    "    image = load_ben_color(path,sigmaX=10)\n",
    "    Image.fromarray(image).save(os.path.join(save_dir, \"{}.png\".format(row['id_code'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebdc0639b6f34b0892a584266f911a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"../input/aptos2019-blindness-detection/test_images_ben_preprocessing_sigmaX10\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "for idx, row in tqdm_notebook(df_test.iterrows()):\n",
    "    path=f\"../input/aptos2019-blindness-detection/test_images/{row['id_code']}.png\"\n",
    "    image = load_ben_color(path,sigmaX=10)\n",
    "    Image.fromarray(image).save(os.path.join(save_dir, \"{}.png\".format(row['id_code'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36] *",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
